{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a70750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: matplotlib in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: opencv-python in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting reportlab\n",
      "  Obtaining dependency information for reportlab from https://files.pythonhosted.org/packages/52/c8/aaf4e08679e7b1dc896ad30de0d0527f0fd55582c2e6deee4f2cc899bf9f/reportlab-4.4.3-py3-none-any.whl.metadata\n",
      "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from reportlab) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer in d:\\users\\vrrs3\\anaconda3\\lib\\site-packages (from reportlab) (2.0.4)\n",
      "Downloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.0 MB 262.6 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.0/2.0 MB 262.6 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/2.0 MB 459.5 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/2.0 MB 701.4 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.0 MB 701.4 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/2.0 MB 793.0 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.3/2.0 MB 884.2 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.0 MB 884.2 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.0 MB 884.2 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/2.0 MB 798.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/2.0 MB 793.8 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.4/2.0 MB 724.0 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.5/2.0 MB 704.5 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 750.8 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 750.8 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 750.8 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 605.3 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.6/2.0 MB 691.4 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/2.0 MB 655.3 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.6/2.0 MB 655.3 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.7/2.0 MB 706.6 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.7/2.0 MB 715.6 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 712.4 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 711.0 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.9/2.0 MB 776.4 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.0/2.0 MB 808.6 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/2.0 MB 887.3 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/2.0 MB 928.8 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/2.0 MB 928.8 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 925.0 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 925.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/2.0 MB 993.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/2.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: reportlab\n",
      "Successfully installed reportlab-4.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install matplotlib opencv-python scikit-learn\n",
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270f9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070bb082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Path to dataset\n",
    "data_dir = \"C:/Users/vrrs3/OneDrive/Desktop\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = datasets.ImageFolder(os.path.join(data_dir, \"Training\"), transform=transform)\n",
    "\n",
    "# Classes\n",
    "class_names = full_dataset.classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Split into train and test (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7eaea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainTumorCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=87616, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BrainTumorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainTumorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64*37*37, 128)\n",
    "        self.fc2 = nn.Linear(128, len(class_names))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*37*37)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = BrainTumorCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7be2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   # <-- add this\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aab4d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     15\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:   \u001b[38;5;66;03m# ✅ use train_loader instead of \"training\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5  # start small for testing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:   # ✅ use train_loader instead of \"training\"\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eddc755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:05<00:00, 1.83MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 129kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 1.14MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 4.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3913\n",
      "Epoch 2/5, Loss: 0.1994\n",
      "Epoch 3/5, Loss: 0.1461\n",
      "Epoch 4/5, Loss: 0.1148\n",
      "Epoch 5/5, Loss: 0.0975\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Define transforms\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normalize (for grayscale MNIST)\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load dataset\n",
    "# ---------------------------\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create DataLoaders\n",
    "# ---------------------------\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define model, loss, optimizer\n",
    "# ---------------------------\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training loop\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:   # ✅ Now it works\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cabb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 960    0    3    1    0    2    7    2    2    3]\n",
      " [   0 1121    3    2    0    0    2    1    6    0]\n",
      " [   6    1 1006    4    2    0    6    1    6    0]\n",
      " [   0    0   12  983    1    0    0    3    7    4]\n",
      " [   0    0    5    0  967    0    5    0    3    2]\n",
      " [   6    2    3   41    4  794   13    1   20    8]\n",
      " [   3    2    3    1    3    1  940    0    5    0]\n",
      " [   0    7   22    3    8    1    2  957    2   26]\n",
      " [   0    0    4    3    7    0    4    3  950    3]\n",
      " [   2    2    1    6   18    0    1    3    6  970]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.95      0.97      0.96      1032\n",
      "           3       0.94      0.97      0.96      1010\n",
      "           4       0.96      0.98      0.97       982\n",
      "           5       0.99      0.89      0.94       892\n",
      "           6       0.96      0.98      0.97       958\n",
      "           7       0.99      0.93      0.96      1028\n",
      "           8       0.94      0.98      0.96       974\n",
      "           9       0.95      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.97      0.96      0.96     10000\n",
      "weighted avg       0.97      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "\n",
    "# ✅ Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # no gradients during evaluation\n",
    "    for images, labels in test_loader:   # <-- use your test_loader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)   # get class with highest probability\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ✅ Generate confusion matrix & classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddeb9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BrainTumorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(BrainTumorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # After 2 pooling layers (224 -> 112 -> 56)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)  \n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (3x224x224 -> 32x112x112)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (32x112x112 -> 64x56x56)\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cec4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def get_model(num_classes=4):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # replace last layer\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a05497cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BrainTumorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(BrainTumorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # After two pooling layers: 224 -> 112 -> 56\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)  \n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (3x224x224 → 32x112x112)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (32x112x112 → 64x56x56)\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = BrainTumorCNN(num_classes=len(train_dataset.classes)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a320ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\vrrs3\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Users\\vrrs3\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\vrrs3/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:02<00:00, 18.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def get_model(num_classes=4):\n",
    "    model = models.resnet18(pretrained=True)  # load pretrained model\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # replace last layer\n",
    "    return model.to(device)\n",
    "\n",
    "model = get_model(num_classes=len(train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f7c3f23",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/vrrs3/OneDrive/Desktop/Training/pituitary/Tr-pi_0021.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m prediction \u001b[38;5;241m=\u001b[39m predict_image(image_path, model, device)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Tumor Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path, model, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m     22\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m class_names[predicted\u001b[38;5;241m.\u001b[39mitem()]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define your class names exactly as in training\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# Image transform (same as training preprocessing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def predict_image(image_path, model, device):\n",
    "    model.eval()  # evaluation mode\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[predicted.item()]  # map index to label\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_path = \"C:/Users/vrrs3/OneDrive/Desktop/Training/pituitary/Tr-pi_0021.jpg\"\n",
    "\n",
    "prediction = predict_image(image_path, model, device)\n",
    "print(f\"Predicted Tumor Type: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b4b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Point this to your TRAIN dataset\n",
    "train_dir = \"C:/Users/vrrs3/OneDrive/Desktop/Training\"\n",
    "train_dataset = datasets.ImageFolder(root=train_dir)\n",
    "\n",
    "# Auto extract class names\n",
    "class_names = train_dataset.classes\n",
    "print(\"Class Names:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dc07dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, device, class_names):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return class_names[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f157302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Predicted Tumor Type: meningioma\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure class_names comes from dataset\n",
    "train_dir = \"C:/Users/vrrs3/OneDrive/Desktop/Training\"\n",
    "train_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_names = train_dataset.classes\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "image_path = \"C:/Users/vrrs3/OneDrive/Desktop/Training/notumor/Tr-no_0020.jpg\"\n",
    "prediction = predict_image(image_path, model, device, class_names)\n",
    "\n",
    "print(f\"Predicted Tumor Type: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4d0c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "dataset = datasets.ImageFolder(\"C:/Users/vrrs3/OneDrive/Desktop/Training\")\n",
    "print(dataset.classes)   # this is the real class order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56046123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioma 1321\n",
      "meningioma 1339\n",
      "notumor 1595\n",
      "pituitary 1457\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for c in os.listdir(\"C:/Users/vrrs3/OneDrive/Desktop/Training\"):\n",
    "    print(c, len(os.listdir(os.path.join(\"C:/Users/vrrs3/OneDrive/Desktop/Training\", c))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "874b1cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tumor Type: meningioma\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the class names in the same order used during training\n",
    "class_names = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
    "\n",
    "# Define the image preprocessing (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match training input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # Imagenet mean\n",
    "                         [0.229, 0.224, 0.225])  # Imagenet std\n",
    "])\n",
    "\n",
    "# Prediction function\n",
    "def predict_image(image_path, model, device):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[predicted.item()]\n",
    "\n",
    "# -------------------------\n",
    "# TEST WITH YOUR OWN IMAGE\n",
    "# -------------------------\n",
    "\n",
    "# Use CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your trained model (make sure weights are loaded already)\n",
    "# Example: model.load_state_dict(torch.load(\"tumor_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Path to your custom image\n",
    "image_path = \"C:/Users/vrrs3/OneDrive/Desktop/Training/notumor/Tr-no_0020.jpg\"\n",
    "\n",
    "# Get prediction\n",
    "prediction = predict_image(image_path, model, device)\n",
    "print(f\"Predicted Tumor Type: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c12c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output size: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Model output size:\", model.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ab9468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset classes:\", train_dataset.classes)  # if using ImageFolder\n",
    "print(\"Number of classes:\", len(train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4261c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
